{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ShIfIRqEJ7g",
    "outputId": "eb5b6546-11f8-427c-84a2-20fa56c8d752"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/aj3281/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package brown to /home/aj3281/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package conll2000 to /home/aj3281/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/aj3281/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data] Downloading package indian to /home/aj3281/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/indian.zip.\n",
      "[nltk_data] Downloading package sinica_treebank to\n",
      "[nltk_data]     /home/aj3281/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data] Downloading package mac_morpho to\n",
      "[nltk_data]     /home/aj3281/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data] Downloading package conll2002 to /home/aj3281/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data] Downloading package cess_cat to /home/aj3281/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cess_cat.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('indian')\n",
    "nltk.download('sinica_treebank')\n",
    "nltk.download('mac_morpho')\n",
    "nltk.download('conll2002')\n",
    "nltk.download('cess_cat')\n",
    "from nltk.corpus import brown, treebank, conll2000, indian, sinica_treebank, mac_morpho, conll2002, cess_cat\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def get_lang_data():\n",
    "  treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "  brown_corpus = brown.tagged_sents(tagset='universal', categories = 'news')\n",
    "  conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "  indian_corpus = indian.tagged_sents()\n",
    "  sinica_treebank_corpus = sinica_treebank.tagged_sents()\n",
    "  conll_2002_corpus = conll2002.tagged_sents()\n",
    "  mac_morpho_corpus = mac_morpho.tagged_sents()\n",
    "  cess_cat_corpus = cess_cat.tagged_sents()\n",
    "  tagged_sentences = treebank_corpus + brown_corpus + conll_corpus + indian_corpus + sinica_treebank_corpus + mac_morpho_corpus + cess_cat_corpus\n",
    "  sentences = []\n",
    "  tags = []\n",
    "\n",
    "  for tagged_sentence in tagged_sentences:\n",
    "    sentence, tag_list = zip(*tagged_sentence)\n",
    "    sentences.append(list(sentence))\n",
    "    tags.append(list(tag_list))\n",
    "  \n",
    "  return sentences, tags\n",
    "\n",
    "sentences, tags = get_lang_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOc8ZmhALX5C",
    "outputId": "75a5618e-89e9-4301-c6d3-8d279da9f9a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wT3d1Ct3L0ER",
    "outputId": "89e16be4-d34d-415b-ab37-7877789a68bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'NOUN',\n",
       " '.',\n",
       " 'NUM',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " '.',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GY6471wcMxfe"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, valid_sentences, train_taggings, valid_taggings = train_test_split(sentences, tags, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i3WrhtNm6Kfe"
   },
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "\n",
    "# for filename in ['en_ewt-ud-train.conllu', 'en_ewt-ud-dev.conllu', 'en_ewt-ud-test.conllu']:\n",
    "#   urllib.request.urlretrieve('https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/' + filename, filename)\n",
    "\n",
    "# with open('en_ewt-ud-train.conllu') as fp:\n",
    "#   for line in fp.readlines()[:10]:\n",
    "#     print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fadmusl1Lzai"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omc39FAJLXRb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1NGuKEZE6LCq"
   },
   "outputs": [],
   "source": [
    "# !pip -q install conllu\n",
    "\n",
    "# import conllu\n",
    "\n",
    "# def load_conllu(filename):\n",
    "#   with open(filename) as fp:\n",
    "#     data = conllu.parse(fp.read())\n",
    "#   sentences = [[token['form'] for token in sentence] for sentence in data]\n",
    "#   taggings = [[token['xpos'] for token in sentence] for sentence in data]\n",
    "#   return sentences, taggings\n",
    "\n",
    "# train_sentences, train_taggings = load_conllu('en_ewt-ud-train.conllu')\n",
    "# valid_sentences, valid_taggings = load_conllu('en_ewt-ud-dev.conllu')\n",
    "# test_sentences, test_taggings = load_conllu('en_ewt-ud-test.conllu')\n",
    "\n",
    "# print(train_sentences[0])\n",
    "# #print(list(zip(train_sentences[42], train_taggings[42])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rFnH4W66P_L",
    "outputId": "692e90dd-de77-4054-cf5f-0025c6e234ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 'PRON'), ('*EXP*-1', 'X'), (\"'s\", 'VERB'), ('interesting', 'ADJ'), ('*', 'X'), ('to', 'PRT'), ('find', 'VERB'), ('that', 'ADP'), ('a', 'DET'), ('lot', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('expensive', 'ADJ'), ('wines', 'NOUN'), ('are', 'VERB'), (\"n't\", 'ADV'), ('always', 'ADV'), ('walking', 'VERB'), ('out', 'ADP'), ('the', 'DET'), ('door', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(train_sentences[182], train_taggings[182])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCw5BN2t6SDA",
    "outputId": "cc169669-0abf-4f1a-c61c-47be4d9662b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different tags: 654\n",
      "176467 N\n",
      "113334 ART\n",
      "104186 NOUN\n",
      "84963 NPROP\n",
      "79323 PREP\n",
      "73126 V\n",
      "62983 ADJ\n",
      "58519 PREP|+\n",
      "51191 ,\n",
      "48931 VERB\n",
      "48549 sps00\n",
      "41634 .\n",
      "37745 ADP\n",
      "34623 ADV\n",
      "33121 DET\n",
      "25241 NUM\n",
      "23410 ncfs000\n",
      "22264 ncms000\n",
      "21077 KC\n",
      "20651 Fc\n",
      "17303 PCP\n",
      "15788 \"\n",
      "15639 PROADJ\n",
      "14823 VAUX\n",
      "13173 np00000\n",
      "13116 da0fs0\n",
      "12762 Fp\n",
      "11909 vmn0000\n",
      "11264 ncmp000\n",
      "11019 cc\n",
      "10947 PROPESS\n",
      "10730 KS\n",
      "10631 PRT\n",
      "10147 NN\n",
      "9811 rg\n",
      "9719 PRON\n",
      "9126 da0ms0\n",
      "8632 CONJ\n",
      "8413 PRO-KS-REL\n",
      "8269 ncfp000\n",
      "7932 vaip3s0\n",
      "7549 spcms\n",
      "7423 da0cs0\n",
      "7148 Nab\n",
      "7120 cs\n",
      "6507 vmip3s0\n",
      "6376 PROSUB\n",
      "6177 sn.e-SUJ\n",
      "5984 pr0cn000\n",
      "5782 )\n",
      "5770 (\n",
      "5309 DE\n",
      "5103 PDEN\n",
      "5083 X\n",
      "5053 aq0cs0\n",
      "5028 :\n",
      "4873 vmp00sm\n",
      "4373 di0ms0\n",
      "4148 da0fp0\n",
      "4115 SYM\n",
      "3708 aq0ms0\n",
      "3677 da0mp0\n",
      "3675 np0000o\n",
      "3347 aq0fs0\n",
      "3306 Nac\n",
      "3288 di0fs0\n",
      "3264 N|AP\n",
      "3250 Fe\n",
      "3173 p0000000\n",
      "3161 Z\n",
      "3096 Fz\n",
      "3061 VM\n",
      "2972 np0000l\n",
      "2918 Nad\n",
      "2882 spcmp\n",
      "2780 VH11\n",
      "2482 W\n",
      "2470 VC2\n",
      "2382 JJ\n",
      "2375 -\n",
      "2281 vmip3p0\n",
      "2255 vaip3p0\n",
      "2216 np0000p\n",
      "2208 Caa\n",
      "2198 aq0cp0\n",
      "2158 rn\n",
      "2154 NNP\n",
      "2145 N|EST\n",
      "2079 Nhaa\n",
      "2077 DM\n",
      "2074 PRP\n",
      "1951 CUR\n",
      "1919 Dd\n",
      "1828 Nca\n",
      "1823 p0300000\n",
      "1766 V|+\n",
      "1732 Ncb\n",
      "1695 aq0msp\n",
      "1679 vmif3s0\n",
      "1632 vsip3s0\n",
      "1617 aq0mp0\n",
      "1615 PRO-KS\n",
      "1531 Nba\n",
      "1401 Fpa\n",
      "1398 Fpt\n",
      "1368 nccs000\n",
      "1339 nccp000\n",
      "1323 pp3cn000\n",
      "1312 aq0fp0\n",
      "1232 Dbb\n",
      "1213 VFM\n",
      "1206 VE2\n",
      "1191 CC\n",
      "1150 ?\n",
      "1148 dd0ms0\n",
      "1134 np0000a\n",
      "1110 aq0fsp\n",
      "1064 Di\n",
      "1062 ;\n",
      "1048 Dbab\n",
      "1038 NNPC\n",
      "933 vmii3s0\n",
      "932 Ncda\n",
      "918 dp3fs0\n",
      "909 VC31\n",
      "901 P21\n",
      "889 dn0cp0\n",
      "878 dd0fs0\n",
      "874 vmg0000\n",
      "871 Ng\n",
      "867 aq0mpp\n",
      "810 di0mp0\n",
      "786 vmsp3s0\n",
      "773 V_11\n",
      "761 vsn0000\n",
      "732 VH13\n",
      "731 Zp\n",
      "729 VA4\n",
      "721 dp3ms0\n",
      "708 Fg\n",
      "703 Dfa\n",
      "695 N|TEL\n",
      "692 N|HOR\n",
      "689 ADV-KS-REL\n",
      "684 !\n",
      "683 vmif3p0\n",
      "681 Neqa\n",
      "665 aq0fpp\n",
      "644 NNC\n",
      "642 Nv4\n",
      "641 Nv1\n",
      "626 Zm\n",
      "611 VC1\n",
      "610 Naa\n",
      "597 Dbaa\n",
      "592 Naeb\n",
      "576 VK1\n",
      "550 DEM\n",
      "546 Neu\n",
      "541 V_2\n",
      "534 Cbca\n",
      "531 QC\n",
      "524 VRB\n",
      "508 QFNUM\n",
      "506 Ta\n",
      "505 di0fp0\n",
      "502 Dab\n",
      "499 ao0ms0\n",
      "497 Dc\n",
      "479 vmsp3p0\n",
      "474 ao0fs0\n",
      "466 RB\n",
      "438 QF\n",
      "435 Fd\n",
      "429 PUNC\n",
      "427 pp3csd00\n",
      "424 VG2\n",
      "424 vsip3p0\n",
      "423 di0cp0\n",
      "421 vsp00sm\n",
      "418 Nep\n",
      "400 VJ3\n",
      "400 '\n",
      "395 pr000000\n",
      "393 di0cs0\n",
      "388 nc00000\n",
      "388 A\n",
      "388 ncmn000\n",
      "387 VA11\n",
      "380 dn0mp0\n",
      "360 vmii3p0\n",
      "350 VJ1\n",
      "342 Dh\n",
      "339 pi0ms000\n",
      "335 dp3mp0\n",
      "322 Fx\n",
      "320 IN\n",
      "319 vaii3s0\n",
      "314 vmip1p0\n",
      "304 VC33\n",
      "300 pd0ns000\n",
      "289 ADV-KS\n",
      "287 dd0mp0\n",
      "278 pp3cp000\n",
      "276 Cbaa\n",
      "274 Ndabd\n",
      "270 P07\n",
      "263 pp3msa00\n",
      "262 pp3nn000\n",
      "252 dp3fp0\n",
      "247 van0000\n",
      "241 Nes\n",
      "237 vmic3s0\n",
      "236 Nhab\n",
      "229 dn0fp0\n",
      "227 vmsi3s0\n",
      "227 dd0fp0\n",
      "226 P31\n",
      "226 Ncc\n",
      "225 VL4\n",
      "221 vsif3s0\n",
      "221 RP\n",
      "220 N|DAT\n",
      "214 Nddc\n",
      "212 vmip1s0\n",
      "208 pp1cp000\n",
      "206 Daa\n",
      "204 N|DAD\n",
      "203 Ncdb\n",
      "202 pr0cs000\n",
      "198 vmp00sf\n",
      "198 pp3fsa00\n",
      "195 PSP\n",
      "194 VH16\n",
      "194 NVB\n",
      "189 VD1\n",
      "188 ao0mp0\n",
      "186 NEG\n",
      "186 vsii3s0\n",
      "183 pi0cs000\n",
      "182 Cab\n",
      "182 NST\n",
      "182 pi0fs000\n",
      "177 P11\n",
      "177 Tc\n",
      "171 VJJ\n",
      "166 VF2\n",
      "166 pr0cp000\n",
      "165 P02\n",
      "165 VNN\n",
      "162 Fit\n",
      "158 VK2\n",
      "157 VA12\n",
      "155 Ndabe\n",
      "152 VH21\n",
      "152 VG1\n",
      "151 XC\n",
      "149 P35\n",
      "147 Naea\n",
      "147 vaip1p0\n",
      "136 pp3csa00\n",
      "135 ADJ|EST\n",
      "133 P61\n",
      "130 VC32\n",
      "127 pn0cp000\n",
      "127 pi0mp000\n",
      "127 vmp00pm\n",
      "123 dn0ms0\n",
      "121 Cbcb\n",
      "119 VJ2\n",
      "117 sn.e\n",
      "117 vssp3s0\n",
      "115 KC|[\n",
      "115 Ndabc\n",
      "114 aq0cn0\n",
      "112 P19\n",
      "111 vaif3s0\n",
      "110 P03\n",
      "110 P23\n",
      "109 Dj\n",
      "109 KC|]\n",
      "109 vaic3s0\n",
      "107 vasp3s0\n",
      "105 dn0fs0\n",
      "105 pp1cs000\n",
      "104 pi0cp000\n",
      "104 pp3ms000\n",
      "104 NLOC\n",
      "103 Nbc\n",
      "103 INTF\n",
      "101 pt0cs000\n",
      "101 VL2\n",
      "100 WQ\n",
      "100 Cbba\n",
      "99 P62\n",
      "98 JVB\n",
      "97 Fia\n",
      "96 VH14\n",
      "93 Ndda\n",
      "91 vmsi3p0\n",
      "91 vaii3p0\n",
      "91 QW\n",
      "91 P06\n",
      "87 VE12\n",
      "87 VF1\n",
      "86 VA13\n",
      "85 VH12\n",
      "84 Nce\n",
      "84 vaip1s0\n",
      "83 pp3mp000\n",
      "78 pp3fpa00\n",
      "78 /\n",
      "77 vsif3p0\n",
      "74 VL1\n",
      "74 Nddb\n",
      "74 vmic3p0\n",
      "73 VB11\n",
      "71 pd0ms000\n",
      "71 pn0ms000\n",
      "66 Ndaad\n",
      "66 pt000000\n",
      "66 Nv2\n",
      "66 vmif1p0\n",
      "65 QO\n",
      "65 vmp00pf\n",
      "65 VAUX|+\n",
      "62 ao0fp0\n",
      "62 Ndaab\n",
      "62 Fs\n",
      "62 ...\n",
      "61 Ndaba\n",
      "60 pp1csn00\n",
      "58 P39\n",
      "58 Ndabb\n",
      "57 Ndabf\n",
      "57 pd0fs000\n",
      "56 pn0mp000\n",
      "54 VH15\n",
      "54 dr0cs0\n",
      "54 Td\n",
      "54 vsii3p0\n",
      "53 VE11\n",
      "52 vap00sm\n",
      "51 P04\n",
      "51 P55\n",
      "51 vsic3s0\n",
      "50 dp1fsp\n",
      "48 Nhac\n",
      "48 pi0fp000\n",
      "47 P63\n",
      "44 VB12\n",
      "44 p010p000\n",
      "44 VH17\n",
      "43 Dk\n",
      "41 di0cn0\n",
      "39 $\n",
      "38 Nv3\n",
      "38 VI2\n",
      "37 Cbbb\n",
      "36 Caa[P1]\n",
      "36 vaic3p0\n",
      "36 dp1msp\n",
      "36 vsg0000\n",
      "35 vasp3p0\n",
      "35 VA2\n",
      "35 \n",
      "34 Caa[P2]\n",
      "34 P47\n",
      "33 vasi3s0\n",
      "33 pd0cs000\n",
      "33 P49\n",
      "32 aq00000\n",
      "31 P37\n",
      "31 RDP\n",
      "30 Nhb\n",
      "30 pp3fs000\n",
      "28 P32\n",
      "28 p010s000\n",
      "28 vssp3p0\n",
      "28 pn0fs000\n",
      "28 vaif3p0\n",
      "27 Ndaaa\n",
      "27 NPROP|+\n",
      "27 vmis3s0\n",
      "26 P41\n",
      "26 vmsp1s0\n",
      "26 vsip1p0\n",
      "26 P59\n",
      "25 vssi3s0\n",
      "25 P16\n",
      "25 pd0mp000\n",
      "24 P50\n",
      "24 dp1fss\n",
      "23 P43\n",
      "23 dp1mss\n",
      "23 vmm03s0\n",
      "22 pn0fp000\n",
      "22 Ndc\n",
      "22 Dg\n",
      "22 vmp0000\n",
      "21 [\n",
      "21 Tb\n",
      "21 UNK\n",
      "19 VI1\n",
      "19 VH22\n",
      "19 pp2cs00p\n",
      "19 P42\n",
      "19 dt0fs0\n",
      "18 PREP|[\n",
      "18 UT\n",
      "17 vmif1s0\n",
      "16 vmip2s0\n",
      "16 dp1fpp\n",
      "16 Dfb\n",
      "16 P26\n",
      "16 vsip1s0\n",
      "16 pp2cs000\n",
      "16 Fat\n",
      "15 dt0ms0\n",
      "15 VA4[+ASP]\n",
      "15 vmm01p0\n",
      "15 vmic1s0\n",
      "14 pd0fp000\n",
      "14 P54\n",
      "14 vasi3p0\n",
      "14 vmii1s0\n",
      "14 P30\n",
      "14 Fh\n",
      "14 vmii1p0\n",
      "13 VD2\n",
      "13 pp3fp000\n",
      "13 Nfg\n",
      "13 P38\n",
      "13 dn0cs0\n",
      "13 VB2\n",
      "12 Nhc\n",
      "12 P13\n",
      "12 V_12\n",
      "12 dp1cpp\n",
      "12 ADV|[\n",
      "12 nccn000\n",
      "12 faa\n",
      "11 Nfa\n",
      "11 INJ\n",
      "11 I\n",
      "11 pt0fs000\n",
      "11 P20\n",
      "11 vmsp1p0\n",
      "10 P58\n",
      "10 ADV|]\n",
      "10 P15\n",
      "10 pt0ms000\n",
      "9 P27\n",
      "9 dp1mpp\n",
      "9 ncfn000\n",
      "9 VC2[+NEG]\n",
      "8 px3ms000\n",
      "8 pp3mpa00\n",
      "8 pt0mp000\n",
      "8 VE2[+NEG]\n",
      "8 ADJ|+\n",
      "8 vmm02s0\n",
      "8 PREP|]\n",
      "8 ADV|+\n",
      "8 ((\n",
      "8 ))\n",
      "8 P60\n",
      "7 vmm03p0\n",
      "7 ADV|EST\n",
      "7 VE2[+DE]\n",
      "7 dt0fp0\n",
      "7 dt0mp0\n",
      "7 PROP\n",
      "7 VI3\n",
      "7 ADV|HOR\n",
      "7 P18\n",
      "7 vmic1p0\n",
      "6 VH11[+ASP]\n",
      "6 ART|+\n",
      "6 VA3\n",
      "6 fca\n",
      "6 fct\n",
      "6 pp2cp00p\n",
      "6 P31[+part]\n",
      "6 pp1cso00\n",
      "6 PREP|+]\n",
      "6 =\n",
      "5 Ndaac\n",
      "5 dp1mps\n",
      "5 z\n",
      "5 pi0cn000\n",
      "5 dp1fps\n",
      "5 VC31[+NEG]\n",
      "5 KS|[\n",
      "5 vmip2p0\n",
      "5 Cbab\n",
      "5 vsm03s0\n",
      "5 px1ms0p0\n",
      "4 RB:?\n",
      "4 pp3cno00\n",
      "4 Dbc\n",
      "4 Neqb\n",
      "4 P14\n",
      "4 sn.e.1n-SUJ\n",
      "4 vssi3p0\n",
      "4 pt0fp000\n",
      "4 vsis3s0\n",
      "4 V|EST\n",
      "4 JJ:?\n",
      "4 VG1[+NEG]\n",
      "4 KS|]\n",
      "4 P24\n",
      "4 ART|EST\n",
      "4 P12\n",
      "4 dd0cs0\n",
      "3 vmsi1p0\n",
      "3 vmis3p0\n",
      "3 p020s000\n",
      "3 P64\n",
      "3 vasi1p0\n",
      "3 vag0000\n",
      "3 px3mp000\n",
      "3 vasp1p0\n",
      "3 sn.e-SUJ.d\n",
      "3 dp2mps\n",
      "3 `\n",
      "3 NN:?\n",
      "3 vsm03p0\n",
      "3 pn0cs000\n",
      "3 vaip2s0\n",
      "3 RP:?\n",
      "3 vaif1p0\n",
      "3 vaic1p0\n",
      "3 VC2[+DE]\n",
      "3 ao0cs0\n",
      "3 vaii1s0\n",
      "3 vsii1s0\n",
      "3 P48\n",
      "3 zp\n",
      "3 vsic3p0\n",
      "3 P46\n",
      "2 VA11[+ASP]\n",
      "2 P55[+part]\n",
      "2 w\n",
      "2 i\n",
      "2 vmif2p0\n",
      "2 aq0000\n",
      "2 vmi0000\n",
      "2 vssp1p0\n",
      "2 px1fs0p0\n",
      "2 VA\n",
      "2 P40\n",
      "2 P22\n",
      "2 PREP|\n",
      "2 sn.e-ATR\n",
      "2 PSP:?\n",
      "2 Str\n",
      "2 Nac[+SPO]\n",
      "2 p020p000\n",
      "2 Head\n",
      "2 vmsi1s0\n",
      "2 VA4[+SPV]\n",
      "2 Nad[+SPO]\n",
      "2 P52\n",
      "2 Nab[+SPO]\n",
      "2 dp2fss\n",
      "2 px3fs000\n",
      "2 fp\n",
      "2 KC|EST\n",
      "2 vaii1p0\n",
      "2 vsii1p0\n",
      "2 sn.co-SUJ\n",
      "2 px3cp0p0\n",
      "2 KC|+\n",
      "2 PREP|EST\n",
      "2 P35[+part]\n",
      "2 VJ3[+NEG]\n",
      "2 P11[+part]\n",
      "1 vsif1s0\n",
      "1 VB11[+ASP]\n",
      "1 \"CC\n",
      "1 IN|EST\n",
      "1 SYM৷\n",
      "1 Nfd\n",
      "1 pd0cp000\n",
      "1 VC31[+DE]\n",
      "1 V|!\n",
      "1 Nfi\n",
      "1 P17\n",
      "1 PPR\n",
      "1 sn.e-ET\n",
      "1 QF:?\n",
      "1 Nfc\n",
      "1 vssp1s0\n",
      "1 sn.e-CD.NFc\n",
      "1 VB11[+SPV]\n",
      "1 vsis3p0\n",
      "1 VH15[+NEG]\n",
      "1 vsic1s0\n",
      "1 VB2[+NEG]\n",
      "1 INJ:?\n",
      "1 vmsp2p0\n",
      "1 vaif2p0\n",
      "1 vmif2s0\n",
      "1 VD2[+NEG]\n",
      "1 vmsp2s0\n",
      "1 dp2mss\n",
      "1 P09\n",
      "1 px3fs0s0\n",
      "1 NNP:?\n",
      "1 sps00-cc\n",
      "1 spcms00\n",
      "1 VJ2[+SPV]\n",
      "1 P01\n",
      "1 NPRO\n",
      "1 VB12[+NEG]\n",
      "1 BM\n",
      "1 vaif1s0\n",
      "1 VC31[+SPV]\n",
      "1 vmn00sm\n",
      "1 PROPESS|+\n",
      "1 CL\n",
      "1 vasp1s0\n",
      "1 VB12[+ASP]\n",
      "1 rg-cc\n",
      "1 VH11[+DE]\n",
      "1 VA3[+ASP]\n",
      "1 sn.e-CD\n",
      "1 QC:?\n",
      "1 VB11[+NEG]\n",
      "1 px1fp0p0\n",
      "1 relatiu\n",
      "1 px3mp00\n",
      "1 pp2cp000\n",
      "1 vaip2p0\n",
      "1 Caa[P1}\n",
      "1 Caa[P2}\n",
      "1 VA11[+NEG]\n",
      "1 vasi1s0\n",
      "1 fg\n",
      "1 VJ1[+NEG]\n",
      "1 vsip2s0\n",
      "1 aqcp00\n",
      "1 vsif1p0\n",
      "1 P25\n",
      "1 P45\n",
      "1 VH11[+NEG]\n",
      "1 vaic1s0\n",
      "1 VK2[+NEG]\n",
      "1 de0ms0\n",
      "1 P08\n",
      "1 P28\n",
      "1 VAUX|!\n",
      "1 VC32[+SPV]\n"
     ]
    }
   ],
   "source": [
    "# use a defaultdict to count the number of occurrences of each tag\n",
    "import collections\n",
    "tagset = collections.defaultdict(int)\n",
    "\n",
    "for tagging in train_taggings:\n",
    "  for tag in tagging:\n",
    "    tagset[tag] += 1\n",
    "\n",
    "print('number of different tags:', len(tagset))\n",
    "\n",
    "# print count and tag sorted by decreasing count\n",
    "for tag, count in sorted(tagset.items(), reverse=True, key=lambda x: x[1]):\n",
    "  print(count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "JnJmPY8J6UoU",
    "outputId": "8a057d56-397f-4be7-ae71-802903e6e67d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 617\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# compute and show histogram for sentence length\n",
    "plt.hist([len(sentence) for sentence in train_sentences], 20)\n",
    "plt.show()\n",
    "\n",
    "# compute max sentence length\n",
    "print('max length:', max([len(sentence) for sentence in train_sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZL6OFL7_A6X2"
   },
   "outputs": [],
   "source": [
    "# train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ct5Cov4H6W_I"
   },
   "outputs": [],
   "source": [
    "# install transformers package\n",
    "!pip -q install transformers\n",
    "\n",
    "# import relevant classes for pretrained tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uNq82yj6Yi3",
    "outputId": "c382e404-cb14-4198-ef63-f828bf51a6c6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debd3bb6b254445f9f48ddb3ab7c6f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abcf7ccdf6d46f98f50c846356f6818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95c6614d2264b2a9544a282a7ed1c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0d8df8ccda4bbeaf7a9813586d8af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.87M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'tok',\n",
       " '##eni',\n",
       " '##zer',\n",
       " 'is',\n",
       " 'soo',\n",
       " '##oo',\n",
       " '##o',\n",
       " 'aw',\n",
       " '##eso',\n",
       " '##me',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer for a specific bert model (bert-base-cased)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# tokenize an example sentence\n",
    "tokenizer.tokenize('This tokenizer is sooooo awesome.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9gvZRHM6aXa",
    "outputId": "fa5d5d5a-2e4b-4832-ad4f-98ab4e44e734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['有', '了', '一對', '小', '犄角']\n",
      "6 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (856 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['就', '在', '自己', '的', '小', '房間', '裡', '打瞌睡']\n",
      "11 12\n",
      "['起步', '甚', '早', '的', '德國', '在', '這', '方面', '卻', '仍', '步履', '蹣跚']\n",
      "16 17\n",
      "['Segundo', 'o', 'dono', 'de', 'o', 'restaurante', ',', 'Fuad', 'Zegaid', ',', '61', ',', 'este', 'é', 'o', 'primeiro', 'caso', 'de', 'assalto', 'em', 'o', 'Dinho`s']\n",
      "27 28\n",
      "['這句', '話', '就', '像', '是', '堅強', '的', '胳臂']\n",
      "10 11\n",
      "['枝頭', '上', '的', '小鳥', '總是', '吱吱喳喳', '的', '叫']\n",
      "11 14\n",
      "['他', '伸開', '胳臂']\n",
      "4 5\n",
      "['日亞航', '目前', '所', '使用', '的', '機種', '主要', '為', '波音', '７４７', '及', 'ＤＣ－１０']\n",
      "21 24\n",
      "['你', '兒子', '賭氣', '不', '吃', '紅蘿蔔', '時', '可以', '連', '餓', '十九個', '不吭一氣']\n",
      "20 22\n",
      "['再', '給', '人', '痛苦', '和', '懊悔']\n",
      "7 8\n",
      "['除了', '有', '維護', '國家', '領土', '主權', '完整', '職責', '的', '基督徒', '總統', '李豋輝']\n",
      "23 24\n",
      "['鼓聲', '咚咚']\n",
      "3 4\n",
      "['外號', '泰山', '的', '黑索汀', '今天', '在', '英國', '保守黨', '黨魁', '選舉', '中', '來勢洶洶']\n",
      "24 25\n",
      "['大福', '靠在', '父親', '的', '床', '邊', '打瞌睡']\n",
      "11 12\n",
      "['電影', '中', '正面', '人物', '的', '行為', '必定', '不會', '和', '觀眾', '的', '認知', '產生', '牴觸']\n",
      "23 24\n",
      "['上個', '星期六', '市場', '上', '一片', '悲觀', '氣氛', '及', '假期', '恐懼症', '瀰漫']\n",
      "21 22\n",
      "['串仔魚', '就', '是', '頭', '小', '而', '身體', '又', '大', '又', '圓', '的', '鮪魚']\n",
      "16 17\n",
      "['嗨咿']\n",
      "1 2\n",
      "['大福', '靠在', '父親', '的', '床', '邊', '打瞌睡']\n",
      "11 12\n",
      "['電影', '中', '正面', '人物', '的', '行為', '必定', '不會', '和', '觀眾', '的', '認知', '產生', '牴觸']\n",
      "23 24\n",
      "['上個', '星期六', '市場', '上', '一片', '悲觀', '氣氛', '及', '假期', '恐懼症', '瀰漫']\n",
      "21 22\n",
      "['串仔魚', '就', '是', '頭', '小', '而', '身體', '又', '大', '又', '圓', '的', '鮪魚']\n",
      "16 17\n",
      "['嗨咿']\n",
      "1 2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def align_tokenizations(sentences, taggings):\n",
    "  bert_tokenized_sentences = []\n",
    "  aligned_taggings = []\n",
    "  count = 0\n",
    "  for sentence, tagging in zip(sentences, taggings):\n",
    "    # first generate BERT-tokenization\n",
    "    bert_tokenized_sentence = tokenizer.tokenize(' '.join(sentence))\n",
    "    # print(bert_tokenized_sentence, sentence)\n",
    "    aligned_tagging = []\n",
    "    current_word = ''\n",
    "\n",
    "    index = 0 # index of current word in sentence and tagging\n",
    "    for token in bert_tokenized_sentence:\n",
    "      current_word += re.sub(r'^##', '', token) # recompose word with subtoken\n",
    "      # print(index, token, len(sentence)) #, sentence)\n",
    "      try: \n",
    "        sentence[index] = sentence[index].replace('\\xad', '') # fix bug in data\n",
    "      except:\n",
    "        print(sentence)\n",
    "        break  \n",
    "        # print(bert_tokenized_sentence)\n",
    "        \n",
    "\n",
    "      # note that some word factors correspond to unknown words in BERT\n",
    "      # print(token, sentence[index].startswith(current_word), sentence[index], current_word)\n",
    "      # assert token == '[UNK]' or sentence[index].startswith(current_word)\n",
    "\n",
    "      if token == '[UNK]' or sentence[index] == current_word: # if we completed a word\n",
    "        current_word = ''\n",
    "        aligned_tagging.append(tagging[index])\n",
    "        index += 1\n",
    "      else: # otherwise insert padding\n",
    "        aligned_tagging.append('<pad>')\n",
    "\n",
    "    # assert len(bert_tokenized_sentence) == len(aligned_tagging)\n",
    "\n",
    "    if(len(aligned_tagging)==len(bert_tokenized_sentence)):\n",
    "        \n",
    "      bert_tokenized_sentences.append(bert_tokenized_sentence)\n",
    "      aligned_taggings.append(aligned_tagging)\n",
    "    else: \n",
    "      print(len(aligned_tagging), len(bert_tokenized_sentence))\n",
    "      count+=1 \n",
    "\n",
    "  return bert_tokenized_sentences, aligned_taggings\n",
    "\n",
    "train_bert_tokenized_sentences, train_aligned_taggings = align_tokenizations(train_sentences, train_taggings)\n",
    "valid_bert_tokenized_sentences, valid_aligned_taggings = align_tokenizations(valid_sentences, valid_taggings)\n",
    "test_bert_tokenized_sentences, test_aligned_taggings = align_tokenizations(valid_sentences, valid_taggings)\n",
    "\n",
    "# print(train_bert_tokenized_sentences[42])\n",
    "# print(train_aligned_taggings[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "uYNv5Ln-6dG4",
    "outputId": "a9ba8274-e712-44a0-ec3c-3156251c6d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num labels: 685\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "import collections\n",
    "\n",
    "label_vocab = collections.defaultdict(lambda: len(label_vocab))\n",
    "label_vocab['<pad>'] = 0\n",
    "\n",
    "def convert_to_ids(sentences, taggings):\n",
    "  sentences_ids = []\n",
    "  taggings_ids = []\n",
    "  for sentence, tagging in zip(sentences, taggings):\n",
    "    sentence_tensor = torch.tensor(tokenizer.convert_tokens_to_ids(['[CLS]'] + sentence + ['SEP'])).long()\n",
    "    tagging_tensor = torch.tensor([0] + [label_vocab[tag] for tag in tagging] + [0]).long()\n",
    "\n",
    "    sentences_ids.append(sentence_tensor.to(device))\n",
    "    taggings_ids.append(tagging_tensor.to(device))\n",
    "  return sentences_ids, taggings_ids\n",
    "\n",
    "train_sentences_ids, train_taggings_ids = convert_to_ids(train_bert_tokenized_sentences, train_aligned_taggings)\n",
    "valid_sentences_ids, valid_taggings_ids = convert_to_ids(valid_bert_tokenized_sentences, valid_aligned_taggings)\n",
    "test_sentences_ids, test_taggings_ids = convert_to_ids(test_bert_tokenized_sentences, test_aligned_taggings)\n",
    "\n",
    "# print(train_sentences_ids[42])\n",
    "# print(train_taggings_ids[42])\n",
    "print('num labels:', len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WyWfx97VOhfs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76196"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_taggings_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "G0dDD67Dx7lq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num labels: 685\n"
     ]
    }
   ],
   "source": [
    "print('num labels:', len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JGAbUbD96fGf"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PosTaggingDataset(Dataset):\n",
    "  def __init__(self, sentences, taggings):\n",
    "    assert len(sentences) == len(taggings)\n",
    "    self.sentences = sentences\n",
    "    self.taggings = taggings\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    return self.sentences[i], self.taggings[i]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BEzvrYbK6g3s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(items):\n",
    "  max_len = max(len(item[0]) for item in items)\n",
    "\n",
    "  sentences = torch.zeros((len(items), max_len), device=items[0][0].device).long().to(device)\n",
    "  taggings = torch.zeros((len(items), max_len)).long().to(device)\n",
    "\n",
    "  for i, (sentence, tagging) in enumerate(items):\n",
    "    sentences[i][0:len(sentence)] = sentence\n",
    "    taggings[i][0:len(tagging)] = tagging\n",
    "\n",
    "  return sentences, taggings\n",
    "\n",
    "\n",
    "x, y = collate_fn([[torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6])], [torch.tensor([1, 2]), torch.tensor([3, 4])]])\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wIjkgyrJ7M8p"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(PosTaggingDataset(train_sentences_ids, train_taggings_ids), batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "valid_loader = DataLoader(PosTaggingDataset(valid_sentences_ids, valid_taggings_ids), batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(PosTaggingDataset(test_sentences_ids, test_taggings_ids), batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uZh3u_f67Otb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 685])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "  def __init__(self, num_labels, embed_size=128, hidden_size=128):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(tokenizer.vocab_size, embed_size, padding_idx=tokenizer.pad_token_id)\n",
    "    self.rnn = nn.GRU(embed_size, hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "    self.decision = nn.Linear(1 * 2 * hidden_size, num_labels) # size output by GRU is number of layers * number of directions * hidden size\n",
    "    self.to(device)\n",
    "  \n",
    "  def forward(self, sentences):\n",
    "    embed_rep = self.embedding(sentences)\n",
    "    word_rep, sentence_rep = self.rnn(embed_rep)\n",
    "    return self.decision(F.dropout(F.gelu(word_rep), 0.3))\n",
    "\n",
    "# check that model works on an arbitrary batch that contains two sentences of length 3\n",
    "rnn_model = RNNClassifier(len(label_vocab))\n",
    "with torch.no_grad():\n",
    "  y = rnn_model(torch.tensor([[0, 1, 2], [3, 4, 5]]).to(device))\n",
    "\n",
    "# the expected shape is (batch size, max sentence length, number of labels)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fMuj7hA97Slz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.551472856308603, 0.0004817324851922003)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perf(model, loader):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  model.eval() # do not apply training-specific steps such as dropout\n",
    "  total_loss = correct = num_loss = num_perf = 0\n",
    "  for x, y in loader:\n",
    "    with torch.no_grad(): # no need to store computation graph for gradients\n",
    "      # perform inference and compute loss\n",
    "      if(x.shape[1]>=512 or x.shape[0]!=64):\n",
    "        continue \n",
    "      y_scores = model(x)\n",
    "      loss = criterion(y_scores.view(-1, len(label_vocab)), y.view(-1)) # requires tensors of shape (num-instances, num-labels) and (num-instances)\n",
    "\n",
    "      # gather loss statistics\n",
    "      total_loss += loss.item()\n",
    "      num_loss += 1\n",
    "\n",
    "      # gather accuracy statistics\n",
    "      y_pred = torch.max(y_scores, 2)[1] # compute highest-scoring tag\n",
    "      mask = (y != 0) # ignore <pad> tags\n",
    "      correct += torch.sum((y_pred == y) * mask) # compute number of correct predictions\n",
    "      num_perf += torch.sum(mask).item()\n",
    "  return total_loss / num_loss, correct.item() / num_perf\n",
    "\n",
    "# without training, accuracy should be a bit less than 2% (chance of getting a label correct)\n",
    "perf(rnn_model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "X4XqlwJR7UbW"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def fit(model, epochs):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = num = 0\n",
    "    for x, y in train_loader:\n",
    "      optimizer.zero_grad() # start accumulating gradients\n",
    "      # print(x.shape)\n",
    "      if(x.shape[1]>=512 or x.shape[0]!=64):\n",
    "        # print(x.shape, \"here\")\n",
    "        continue \n",
    "      else: \n",
    "        # print(x.shape, \"222222\")  \n",
    "        y_scores = model(x)\n",
    "        loss = criterion(y_scores.view(-1, len(label_vocab)), y.view(-1))\n",
    "        loss.backward() # compute gradients though computation graph\n",
    "        optimizer.step() # modify model parameters\n",
    "        total_loss += loss.item()\n",
    "        num += 1\n",
    "    print(1 + epoch, total_loss / num, *perf(model, valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OfV0qauI7W80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.20993497718738605 0.1092822381063615 0.8618960552678542\n",
      "2 0.09186533502605645 0.09118140165972982 0.8858567721733799\n",
      "3 0.08077287392745792 0.08704039385438268 0.8908310250318418\n",
      "4 0.07749751727666533 0.08861318703854296 0.8871797117633964\n",
      "5 0.07819280645609657 0.09248456298397277 0.8877034133666166\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNNClassifier(len(label_vocab))\n",
    "fit(rnn_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "I1zhzatj7Yni"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 685])\n"
     ]
    }
   ],
   "source": [
    "class LinearProbeRandom(nn.Module):\n",
    "  def __init__(self, num_labels):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(tokenizer.vocab_size, 768)\n",
    "    self.probe = nn.Linear(768, num_labels)\n",
    "    self.to(device)\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.probe.parameters()\n",
    "  \n",
    "  def forward(self, sentences):\n",
    "    with torch.no_grad(): # embeddings are not trained\n",
    "      word_rep = self.embedding(sentences)\n",
    "    return self.probe(word_rep)\n",
    "\n",
    "# the model should return a tensor of shape (batch size, sequence length, number of labels)\n",
    "random_model = LinearProbeRandom(len(label_vocab))\n",
    "with torch.no_grad():\n",
    "  y = random_model(torch.tensor([[0, 1, 2], [3, 4, 5]]).to(device))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-0v6KL6M7axy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6556266986493823 0.5447546104867447 0.4236143804445807\n",
      "2 0.5275628868914858 0.5366836116852494 0.4519982774414166\n",
      "3 0.5253548510844195 0.526930922784176 0.44412633069475316\n",
      "4 0.5244107322225088 0.5411739227584171 0.42533511428373316\n",
      "5 0.5291395352233814 0.5299316490362138 0.4557244052246078\n"
     ]
    }
   ],
   "source": [
    "random_model = LinearProbeRandom(len(label_vocab))\n",
    "fit(random_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SClH8FAE7cK0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326d00f81a644afdb9aa274d573f0be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 685])\n"
     ]
    }
   ],
   "source": [
    "class LinearProbeBert(nn.Module):\n",
    "  def __init__(self, num_labels):\n",
    "    super().__init__()\n",
    "    self.bert = AutoModel.from_pretrained('bert-base-multilingual-cased')\n",
    "    self.probe = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    self.to(device)\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.probe.parameters()\n",
    "  \n",
    "  def forward(self, sentences):\n",
    "    with torch.no_grad(): # no training of BERT parameters\n",
    "      word_rep, sentence_rep = self.bert(sentences, return_dict=False)\n",
    "    return self.probe(word_rep)\n",
    "\n",
    "# the model should return a tensor of shape (batch size, sequence length, number of labels)\n",
    "bert_model = LinearProbeBert(len(label_vocab))\n",
    "y = bert_model(torch.tensor([[0, 1, 2], [3, 4, 5]]).to(device))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "02NsHaE_7d4K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.15301401384394883 0.1018606998016916 0.9014400881716428\n",
      "2 0.12271086857477322 0.10312802214553665 0.8973563105130816\n",
      "3 0.12228443542739006 0.10489413854642417 0.9076569918725891\n",
      "4 0.12329749285483159 0.10397045831976053 0.9030458631222834\n",
      "5 0.12352240785076145 0.10512989063080647 0.9035257708632135\n"
     ]
    }
   ],
   "source": [
    "bert_model = LinearProbeBert(len(label_vocab))\n",
    "fit(bert_model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Qvd6mmNo7gwn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN representation (supervised) 0.09265619292287959 0.8878001248125075\n",
      "RANDOM representation (unsupervised) 0.5299316490362138 0.4557244052246078\n",
      "BERT representation (unsupervised) 0.10512989063080647 0.9035257708632135\n"
     ]
    }
   ],
   "source": [
    "print('RNN representation (supervised)', *perf(rnn_model, test_loader))\n",
    "print('RANDOM representation (unsupervised)', *perf(random_model, test_loader))\n",
    "print('BERT representation (unsupervised)', *perf(bert_model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJE3nLsr7ioB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
